{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnGW0UiNkp9BTISv1vaYrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitrogoose/MACHINE_LEARNING/blob/main/ASS3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNezxDD8Wc1k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/USA_Housing.csv'  # Replace with actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# a) Divide dataset into input features and output variable\n",
        "X = data.drop(columns=['Price'])\n",
        "y = data['Price']\n",
        "\n",
        "# b) Scale input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# c) Divide input and output features into 5 folds\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_beta = None\n",
        "max_r2_score = -np.inf\n",
        "\n",
        "# d) 5 iterations for cross-validation\n",
        "for train_index, test_index in kf.split(X_scaled):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    beta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ y_train\n",
        "    y_pred = X_test @ beta\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    if r2 > max_r2_score:\n",
        "        max_r2_score = r2\n",
        "        best_beta = beta\n",
        "\n",
        "print(\"Best Beta Coefficients:\", best_beta)\n",
        "print(\"Maximum R2 Score:\", max_r2_score)\n",
        "\n",
        "# e) Train regressor on 70% of data and test on remaining 30%\n",
        "split = int(0.7 * len(X_scaled))\n",
        "X_train, X_test = X_scaled[:split], X_scaled[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "beta_final = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ y_train\n",
        "y_pred_final = X_test @ beta_final\n",
        "r2_final = r2_score(y_test, y_pred_final)\n",
        "\n",
        "print(\"Final R2 Score on Test Set:\", r2_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.001, 0.01, 0.1, 1]\n",
        "X_train, X_temp = X_scaled[:int(0.56 * len(X_scaled))], X_scaled[int(0.56 * len(X_scaled)):]\n",
        "y_train, y_temp = y[:int(0.56 * len(y))], y[int(0.56 * len(y)):]\n",
        "\n",
        "X_val, X_test = X_temp[:int(0.14 * len(X_scaled))], X_temp[int(0.14 * len(X_scaled)):]\n",
        "y_val, y_test = y_temp[:int(0.14 * len(y))], y_temp[int(0.14 * len(y)):]\n",
        "\n",
        "best_beta = None\n",
        "best_r2 = -np.inf\n",
        "\n",
        "for lr in learning_rates:\n",
        "    beta = np.zeros(X_train.shape[1])\n",
        "    for _ in range(1000):\n",
        "        gradient = -2 * X_train.T @ (y_train - X_train @ beta) / len(y_train)\n",
        "        beta -= lr * gradient\n",
        "\n",
        "    y_val_pred = X_val @ beta\n",
        "    r2_val = r2_score(y_val, y_val_pred)\n",
        "\n",
        "    if r2_val > best_r2:\n",
        "        best_r2 = r2_val\n",
        "        best_beta = beta\n",
        "\n",
        "y_test_pred = X_test @ best_beta\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Best R2 on Validation Set:\", best_r2)\n",
        "print(\"Best R2 on Test Set:\", r2_test)\n",
        "print(\"Best Beta Coefficients:\", best_beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "K838Gs5FWeEY",
        "outputId": "1e7573fb-db08-413b-e012-61bf6cbcee35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3ae9e7f25a97>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.56\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.56\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.56\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.56\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.14\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.14\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'\n",
        "columns = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\", \"num_doors\",\n",
        "           \"body_style\", \"drive_wheels\", \"engine_location\", \"wheel_base\", \"length\", \"width\",\n",
        "           \"height\", \"curb_weight\", \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
        "           \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\",\n",
        "           \"highway_mpg\", \"price\"]\n",
        "\n",
        "data = pd.read_csv(url, names=columns)\n",
        "data.replace('?', np.nan, inplace=True)\n",
        "data.fillna(data.mean(numeric_only=True), inplace=True)\n",
        "data.dropna(subset=['price'], inplace=True)\n",
        "\n",
        "data['num_doors'] = data['num_doors'].replace({'two': 2, 'four': 4})\n",
        "data['num_cylinders'] = data['num_cylinders'].replace({\n",
        "    'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'eight': 8, 'twelve': 12\n",
        "})\n",
        "\n",
        "data['fuel_system'] = data['fuel_system'].apply(lambda x: 1 if 'pfi' in str(x) else 0)\n",
        "data['engine_type'] = data['engine_type'].apply(lambda x: 1 if 'ohc' in str(x) else 0)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "for col in ['make', 'aspiration', 'engine_location', 'fuel_type']:\n",
        "    data[col] = encoder.fit_transform(data[col])\n",
        "\n",
        "data = pd.get_dummies(data, columns=['body_style', 'drive_wheels'], drop_first=True)\n",
        "\n",
        "X = data.drop(columns=['price'])\n",
        "y = data['price'].astype(float)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "r2_before_pca = regressor.score(X_test, y_test)\n",
        "\n",
        "pca = PCA(n_components=10)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor_pca = LinearRegression()\n",
        "regressor_pca.fit(X_train_pca, y_train_pca)\n",
        "r2_after_pca = regressor_pca.score(X_test_pca, y_test_pca)\n",
        "\n",
        "print(\"R2 Score Before PCA:\", r2_before_pca)\n",
        "print(\"R2 Score After PCA:\", r2_after_pca)\n"
      ],
      "metadata": {
        "id": "sYzJhfGcWgn1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}